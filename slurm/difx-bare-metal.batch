#!/bin/bash
##SBATCH -n 48 # Number of cores requested -- commandline
##SBATCH -N 1 # Number of nodes requested -- commandline
#SBATCH -t 10000 # Runtime in minutes
#SBATCH -p blackhole,unrestricted,shared # Partition to submit to
#SBATCH --mem-per-cpu=1000 # Memory per cpu in MB (see also --mem-per-cpu)
#SBATCH --open-mode=append
#SBATCH -o job_%j.out # Standard out goes to this file
#SBATCH -e job_%j.err # Standard err goes to this filehostname
#SBATCH --constraint="holyhdr" # holyscratch is only reliable with this constraint

. ~/greg.sh
cd ~/github/difx-try4-with-ipp
DIFXROOT_OVERRIDE=$PWD . setup.bash
LD_LIBRARY_PATH=$LD_LIBRARY_PATH:/n/home02/glindahl/.conda/envs/eht38v2/lib  # for libjpeg
source $DIFXROOT/bin/hops.bash
echo LD_LIBRARY_PATH = $LD_LIBRARY_PATH

cd $SLURM_SUBMIT_DIR

echo SLURM_CPUS_ON_NODE = $SLURM_CPUS_ON_NODE
echo SLURM_JOB_NODELIST = $SLURM_JOB_NODELIST

cd $DIFX_BAND

../slurm-to-machines.py $DIFX_INPUT > machines.$DIFX_INPUT
echo "slurm machines file is"
cat machines.$DIFX_INPUT

echo "running genmachines"
export DIFX_MACHINES=machines.$DIFX_INPUT
../genmachines-multinode.py $DIFX_INPUT
#rm machines.$DIFX_INPUT
BASE=`basename $DIFX_INPUT .input`
echo difx machines
cat $BASE.machines
echo difx threads
cat $BASE.threads

# --oversubscribe -- probably not needed. when set, it implies "--mca mpi_yield_when_idle 1"
# --map-by node means round robin scheduling of things in the machines file,
#   instead of openmpi asking slurm how many slots each node has
# --mca rmaps seq means to use the machines file in exact order
DIFX_MPIRUNOPTIONS="--oversubscribe --map-by node --mca rmaps seq"
# and we want our Conda / DiFX / HOPS stuff to flow to the compute nodes
DIFX_MPIRUNOPTIONS="$DIFX_MPIRUNOPTIONS -x LD_LIBRARY_PATH"

export DIFX_MPIRUNOPTIONS

echo DIFX_MPIRUNOPTIONS = $DIFX_MPIRUNOPTIONS

if [ ! -f runmpifxcorr.$DIFX_VERSION ]; then
  ln -s `which mpifxcorr` runmpifxcorr.$DIFX_VERSION
fi

startdifx --dont-calc -f --nomachines $DIFX_INPUT
