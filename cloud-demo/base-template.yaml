cluster_name: greg-cloud-mri

max_workers: 6  # cluster_wide, also default for available_node_types...max_workers

provider:
    type: gcp
    region: us-central1
    availability_zone: us-central1-c
    project_id: eht-cloud # Globally unique project id

available_node_types:
    ray_head:
        resources: {"CPU": 112}
        node_config:
            #machineType: n1-standard-2
            machineType: c2d-highcpu-112
            disks:
                - boot: true
                  autoDelete: true
                  type: PERSISTENT
                  initializeParams:
                      diskSizeGb: 50
                      sourceImage: projects/deeplearning-platform-release/global/images/family/common-cpu
        max_workers: 0
    ray_worker:
        min_workers: 1
        max_workers: 1
        resources: {"CPU": 112}
        node_config:
            machineType: c2d-highcpu-112
            disks:
                - boot: true
                  autoDelete: true
                  type: PERSISTENT
                  initializeParams:
                      diskSizeGb: 50
                      sourceImage: projects/deeplearning-platform-release/global/images/family/common-cpu
            schedulingConfig:
                 preemptible: true

file_mounts: {
    #"./apt-get-wait.sh": './apt-get-wait.sh',
    "./hello-world.c": "./hello-world.c",
    #./a.out": "./a.out",  # openmpi wants this on all nodes... but this kills ray when you do it
    "./hello-world-driver.py": "./hello-world-driver.py",
    "./fixup-ssh.py": "./fixup-ssh.py",
    "./ray-prep.py": "./ray-prep.py",
}

setup_commands:
    - pip install paramsurvey[ray]
    - pip install git+https://github.com/Smithsonian/paramsurvey_multimpi.git
    # ray runs "apt get -y upgrade" in the background so we can't get the lock until that finishes... 4-5 minutes
    # fortunately the ray image does have openmpi installed
    #- sudo apt-get install openmpi-bin libopenmpi-dev
    - mkdir -p ~/bucket
    - gcsfuse greg-transfer-test-nov-2021 ~/bucket || /bin/true  # assume a failure means it's already mounted?
    - mpicc hello-world.c  # on all nodes

head_setup_commands:
    - (yes '' | ssh-keygen -N '') || /bin/true  # will not overwrite. my code propagates this public key to workers.
    - sudo /sbin/sysctl -w net.core.somaxconn=2048 # gcs server can overrun the usual 128

worker_setup_commands:
    - (yes '' | ssh-keygen -N '') || /bin/true  # will not overwrite. intended to create the directory with proper permissions.

head_start_ray_commands:
    - ray disable-usage-stats >/dev/null 2>&1 || /bin/true
    - ray stop
    # ulimit needs to be with the ray start because each line is a separate ssh
    # autoscaler env vars need to be here to have an effect
    - >-
        ulimit -n 1048576;
        AUTOSCALER_MAX_CONCURRENT_LAUNCHES=200 AUTOSCALER_MAX_LAUNCH_BATCH=20 ray start
        --head
        --port=6379
        --object-manager-port=8076
        --autoscaling-config=~/ray_bootstrap_config.yaml

worker_start_ray_commands:
    - ray disable-usage-stats >/dev/null 2>&1 || /bin/true
    - ray stop
    - >-
        ray start
        --address=$RAY_HEAD_IP:6379
        --object-manager-port=8076

head_node_type: ray_head

auth:
    ssh_user: ubuntu
