export GOOGLE_APPLICATION_CREDENTIALS="/home/glindahl/cloud-demo/eht-cloud-b038a415dc0d.json"


How did I manage to spend 20 hours on this task?

google changed "something" about disk image names
- fell back to using an image from the ray folks
realized I had accidentally made 2 repos for one of my software pieces
- merged them
had to figure out syntax to customize the disk image from ray at cluster startup
ray has some odd ideas about how ssh should be set up on a cluster
- which disagrees with what openmpi expects
- wrote a script which mostly works around that
- ok, and a second script for the rest of it
openmpi startup weirdness relating to environment variables on ubuntu
- read the source, added a weird flag to fix
bugs in my mpi adapter for paramsurvey not previously caught by testing
- ended up finding an unhandled case, that fixed it

discovered google cloud has a "quota" system for limiting the size of clusters
- 2 business day response time
- current quota is 500 cores (4 nodes) of the best compute node, 4096 cores (68 nodes) of the less best


difx
- have code that creates the appropriate mpi "machines file"
- spreads the bucket reading over all of the nodes
- need to pack up a compiled version of difx
- take Jan Wagner's skeleton of config files

pip install git+https://github.com/Smithsonian/paramsurvey_multimpi.git

setup_commands: []  # a list
{head,worker}_setup_commands: []  # also lists



TODO add a log entry for ^C

ssh configuration, am I not waiting long enough for the 'added by google' lines

that one last bug with being surprised to be exiting and mpi


with leader=2 follow=112 wanted=114 still gets too few cores on 2nd and 3rd run
sums only shows the leader
machinefile agrees
return from server has no followers

GREG making return with state running and fkeys is ['ray-gce-ray-minimal-greg-worker-0aafc0d2-compute_20765']
 before schedule: waiting
  schedule: wanted 112 cores in addition to leader cores 2
   reschedule, after existing follower cores we still want 0
   GREG did not need more followers
   GREG existing followers are ['ray-gce-ray-minimal-greg-worker-0aafc0d2-compute_20765']
    schedule, wanted 0 fkeys []
 re-scheduled jobnumber 1
 GREG rescheduling
 GREG rescheduling: fkeys was already ['ray-gce-ray-minimal-greg-worker-0aafc0d2-compute_20765']
 GREG rescheduling: adding new fkeys []
 GREG after extend, here is the summary of fkeys
 GREG  ray-gce-ray-minimal-greg-worker-0aafc0d2-compute_20765 cores 112
  after schedule: scheduled
GREG making return with state scheduled and fkeys is ['ray-gce-ray-minimal-greg-worker-0aafc0d2-compute_20765']


      checking ray-gce-ray-minimal-greg-worker-0aafc0d2-compute_21960
         not assigned or running, but  exiting
  not all followers still exist, so triggering a new schedule
    old valid fkeys: ['ray-gce-ray-minimal-greg-worker-0aafc0d2-compute_21401']
    new valid fkeys: []
before schedule: scheduled
    schedule: wanted 112 cores in addition to leader cores 2
GREG needed 112 more followers
 ff: did not find enough cores
 GREG new followers are None
  failed to schedule
   after schedule: scheduled
   GREG making return with state scheduled and fkeys is []




BUG
  jobnumber isn't getting set to an integer? but it does in test

ok try for 2 equal sized nodes -- goddamnit somehow it though there were 224 slots on one node, maybe the nodename had _ in it
still have to fixup ssh

use RAY_HEAD_IP to ssh-keyscan or ssh-copy-id ?

(seems impossible) tell orte to not check hostkeys







ssh without hostkey checking

ssh -o StrictHostKeyChecking=no

ray get-worker-ids hello-world.yaml
ray get-worker-ips ray_bootstrap_config.yaml  # shows the external ip and apparently that can't be sshed to

import ray
ray.init(address='auto')
print([x['NodeName'] for x in ray.nodes()])
['10.128.0.121', '10.128.0.34']



# Quota 'C2D_CPUS' exceeded.  Limit: 500.0 in region us-central1.
gcloud compute regions describe us-central1
- limit: 4096.0
metric: C2_CPUS
usage: 4.0
- limit: 500.0
metric: C2D_CPUS
usage: 0.0

gcloud compute project-info describe --project eht-cloud
nothing useful -- apparently all the defaults

zone list
us-central1-[abcf]
  [acf]: c2d
us-east1-[bcd]
  [cd]: c2d
us-east4-[abc]
  [abc]: c2d
us-east5-[abc]
  c: no c2
# us-south1 no c2 or c2d
us-west1-[abc]
us-west2-[abc]
us-west3-[abc]
us-west4-[abc]






https://cloud.google.com/docs/quota#requesting_higher_quota=

