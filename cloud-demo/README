export GOOGLE_APPLICATION_CREDENTIALS="/home/glindahl/cloud-demo/eht-cloud-b038a415dc0d.json"


How did I manage to spend 20 hours on this task?

google changed "something" about disk image names
- fell back to using an image from the ray folks
realized I had accidentally made 2 repos for one of my software pieces
- merged them
had to figure out syntax to customize the disk image from ray at cluster startup
ray has some odd ideas about how ssh should be set up on a cluster
- which disagrees with what openmpi expects
- wrote a script which mostly works around that
- ok, and a second script for the rest of it
openmpi startup weirdness relating to environment variables on ubuntu
- read the source, added a weird flag to fix
bugs in my mpi adapter for paramsurvey not previously caught by testing
- ended up finding an unhandled case, that fixed it



pip install git+https://github.com/Smithsonian/paramsurvey_multimpi.git

setup_commands: []  # a list
{head,worker}_setup_commands: []  # also lists



post hello-world

* how do I update the persistant disk? Or I can copy over files in the yaml
* make a skeleton of files for difx based on working/* and the golden scans
* add a mount command for the bucket to startup

pip install google-api-python-client  # it's in the ray venv
pip install boto3  # not an explcit dependency, needed by "ray exec"

sudo apt-get install apt-transport-https ca-certificates gnupg



with leader 2 follower 112 wanted 114, got too few cores
leader 112 wanted 112 is fine
leader 2 follower 112 wanted 112 got too few cores
/tmp/machinefile_* looks ok

Ahah. it works the first time??
ok let's run wanted 30 repeatedly OK
wanted 70 repeatedly OK
these are running everything on the big node

added prints in client on leader node, but that does nothing
added a unit test, no lucky

running 2+68 = 70 only starts one
then it hangs with 4 finished 2 waiting
so apparently the followers all exited and it's just 2 leaders left?

2+68 once does exit but gets the ORTE hostkey verification problem
the machinefile has 2 lines as expected
OK the fat node needed a "Y" to ssh to itself
after that it gets fatal error in MPI_INIT


TODO add a log entry for ^C






AHAH

ubuntu@ray-gce-ray-minimal-greg-head-11e47bc7-compute:~$ ssh ray-gce-ray-minimal-greg-worker-0aafc0d2-compute mpirun --help
--------------------------------------------------------------------------
Sorry!  You were supposed to get help about:
    orterun:usage
But I couldn't open the help file:
    /tmp/usr/share/openmpi/help-orterun.txt: No such file or directory.  Sorry!
-----------------------------------------------------------------------

but

ubuntu@ray-gce-ray-minimal-greg-worker-0aafc0d2-compute:~$ mpirun --help
mpirun (Open MPI) 4.0.2

ubuntu@ray-gce-ray-minimal-greg-head-11e47bc7-compute:~$ ssh ray-gce-ray-minimal-greg-worker-0aafc0d2-compute printenv | grep OPAL
... is empty, should be OPAL_PREFIX=/usr

but hello-world-env.c did print OPAL_PREFIX=/usr on both nodes







with leader=2 follow=112 wanted=114 still gets too few cores on 2nd and 3rd run
sums only shows the leader
machinefile agrees
return from server has no followers

GREG making return with state running and fkeys is ['ray-gce-ray-minimal-greg-worker-0aafc0d2-compute_20765']
 before schedule: waiting
  schedule: wanted 112 cores in addition to leader cores 2
   reschedule, after existing follower cores we still want 0
   GREG did not need more followers
   GREG existing followers are ['ray-gce-ray-minimal-greg-worker-0aafc0d2-compute_20765']
    schedule, wanted 0 fkeys []
 re-scheduled jobnumber 1
 GREG rescheduling
 GREG rescheduling: fkeys was already ['ray-gce-ray-minimal-greg-worker-0aafc0d2-compute_20765']
 GREG rescheduling: adding new fkeys []
 GREG after extend, here is the summary of fkeys
 GREG  ray-gce-ray-minimal-greg-worker-0aafc0d2-compute_20765 cores 112
  after schedule: scheduled
GREG making return with state scheduled and fkeys is ['ray-gce-ray-minimal-greg-worker-0aafc0d2-compute_20765']


      checking ray-gce-ray-minimal-greg-worker-0aafc0d2-compute_21960
         not assigned or running, but  exiting
  not all followers still exist, so triggering a new schedule
    old valid fkeys: ['ray-gce-ray-minimal-greg-worker-0aafc0d2-compute_21401']
    new valid fkeys: []
before schedule: scheduled
    schedule: wanted 112 cores in addition to leader cores 2
GREG needed 112 more followers
 ff: did not find enough cores
 GREG new followers are None
  failed to schedule
   after schedule: scheduled
   GREG making return with state scheduled and fkeys is []




BUG
  jobnumber isn't getting set to an integer? but it does in test

ok try for 2 equal sized nodes -- goddamnit somehow it though there were 224 slots on one node, maybe the nodename had _ in it
still have to fixup ssh

use RAY_HEAD_IP to ssh-keyscan or ssh-copy-id ?

(seems impossible) tell orte to not check hostkeys







ssh without hostkey checking

ssh -o StrictHostKeyChecking=no

ray get-worker-ids hello-world.yaml
ray get-worker-ips ray_bootstrap_config.yaml  # shows the external ip and apparently that can't be sshed to

import ray
ray.init(address='auto')
print([x['NodeName'] for x in ray.nodes()])
['10.128.0.121', '10.128.0.34']



# Quota 'C2D_CPUS' exceeded.  Limit: 500.0 in region us-central1.
gcloud compute regions describe us-central1
- limit: 4096.0
metric: C2_CPUS
usage: 4.0
- limit: 500.0
metric: C2D_CPUS
usage: 0.0

gcloud compute project-info describe --project eht-cloud
nothing useful -- apparently all the defaults

zone list
us-central1-[abcf]
  [acf]: c2d
us-east1-[bcd]
  [cd]: c2d
us-east4-[abc]
  [abc]: c2d
us-east5-[abc]
  c: no c2
# us-south1 no c2 or c2d
us-west1-[abc]
us-west2-[abc]
us-west3-[abc]
us-west4-[abc]






https://cloud.google.com/docs/quota#requesting_higher_quota=

